---
title: "The Effect of Class Sizing on First Grade Students Math Performance"
author: "Feini Pek"
date: "03/17/2024 "
output: 
  html_document:
    theme : readable
    toc: true
    toc_depth : 5
    number_sections: no
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1. Abstract

This investigation addressed two primary questions regarding the influence of various class types on the mathematical performance of grade 1 students: (a) whether differences exist in math scaled scores among 1st-grade students across different class types, and (b) which class type correlates with the highest math scaled scores among 1st-grade students. The study included 6,829 students who participated in Tennessee's Project STAR, during their first grade. Upon inspecting the dataset, patterns of missing data were observed, and addressed through data imputation. An issue of heterogeneity also surfaced as student race and free-lunch status were unevenly distributed across various school types, and accounted for through variable incorporation. Additionally, noncompliance emerged as a challenge, with actual class sizes often diverging from their assigned class types. To mitigate this, we substituted class type with true class size in sensitivity analysis. The outcomes consistently revealed the existence of differences in scaled math scores among different class sizes, with small classes consistently demonstrating the highest mean math scores across all models.

### 2. Introduction

In the educational journey, early academic achievements wield significant influence, as evidenced by Bloom (1964) and Garnier, Stein, and Jacobs (1997), who underscore the enduring impact of grade 1 mathematics and reading performance on long-term outcomes, including high school graduation rates. While some studies suggest that smaller class sizes lead to enhanced learning outcomes, others contend that the benefits may be negligible or even nonexistent. The present investigation stems from a growing interest in understanding the implications of class size variation within the context of first-grade mathematics performance. The data for this analysis are drawn from Tennessee's Project STAR (Student/Teacher Achievement Ratio), a significant educational initiative designed to explore the effects of class size reduction on student outcomes. The dataset comprises information from 11,601 participants. With a focus on 6,829 first-grade students who participated in the project, we aim to address two primary research questions:

- 1). Is there differences in math scaled scores among first-grade students across different class types?
- 2). Which class type is associated with the highest math scaled scores among first-grade students?

Key variables involved in this analysis include math scaled scores, class type (small, regular, regular + aide), school ID, school urbanicity, and student demographic characteristics such as race and free lunch eligibility. Possible hypotheses, informed by literature such as the Glass Meta-Analysis (1982) and studies by Finn and Gerber (2005), suggest that reduced class size may have a noticeable effect on student achievement, particularly in mathematics and reading. The results of this data analysis hold real-world implications for educational policymakers, educators, and stakeholders, informing decisions regarding classroom organization and resource allocation in early education settings.

### 3. Background

The Tennessee Student Teacher Achievement Ratio (STAR) project, conducted in 1985, sought to investigate the impact of class size reduction on student achievement in early grades. The collected data was as part of a randomized controlled trial (RCT) within Tennessee public schools, and sourced from Harvard Dataverse in this analysis. The STAR project utilized a stratified random sampling approach, with K-3 students in Tennesse as the sample. Within each school, students were randomly assigned to one of three class size conditions: small class (13-17 students), regular class (22-25 students), or regular class with teacher aides. In our analysis, the primary dependent variable is scaled math scores for grade 1 (g1tmathss), while the main independent variable of interest is class type (g1classtype), distinguishing classroom environments as small class (1), regular class (2), or regular class with teacher aides (3); and school ID (g1schid), a unique 6-digit identifier with the first 3 digits representing the district identifier, additional variables considered include school urbanicity (G1surban), student race, eligibility for free lunch (G1freelunch), and class size (G1classsize). 

Research on the relationship between class size reduction and student achievement has been extensive and has yielded mixed findings. Several studies have reported positive effects of smaller class sizes on academic outcomes, particularly in the early grades. For example, a study of this project by Krueger and Whitmore (2001) found that students in smaller classes outperformed their peers in larger classes on standardized tests. Similarly, another study concluded that reducing class sizes in early grades led to improved student achievement and long-term benefits, such as higher graduation rates and college attendance (Word et al., 1990). However, other research has questioned the sustainability and cost-effectiveness of class size reduction policies. A meta-analysis by Hattie (2009) suggested that while small class sizes may have short-term benefits, the effects tend to diminish over time and may not justify the associated costs. Some studies also found no significant differences in academic outcomes between small and large classes, particularly in older grades or for certain student subgroups (Dynarski et al., 2013). Overall, while there is evidence supporting the positive impact of class size reduction on student achievement, the effectiveness of such interventions may depend on various contextual factors, including school resources, teacher quality, and instructional practices.

### 4. Experimental Design

The experimental design of Project STAR involved the random assignment of students to different class sizes—small classes (13-17 students per teacher), regular-sized classes (22-25 students per teacher), or regular-sized classes with a full-time teacher aide—enabling a comparison of academic performance over time. This longitudinal study tracked students from kindergarten through third grade, employing standardized tests in reading and mathematics to assess academic achievement annually. Extensive data collection on student demographics, teacher characteristics, and school resources facilitated control for potential confounding factors.

The randomization in STAR ensures that observed differences in academic achievement between class size conditions can be attributed to the intervention (class size reduction) rather than pre-existing differences between students. Its longitudinal study design tracks students' academic progress over multiple years, offering insights into the long-term effects of class size reduction on academic achievement. Additionally, STAR project ensures that each participating school had enough students to allocate for each class type. This allows more control over factors like teacher quality and school resources, ensuring that observed differences are attributable to class size reduction.

While the design of Project STAR has been praised for its rigor and longitudinal nature, it is not without its limitations. Ethical considerations arise from the random assignment of students to different class sizes, with some arguing that this practice may unfairly disadvantage students assigned to larger classes. Additionally, the limited generalizability of the study's findings is a point of contention, as Project STAR was conducted exclusively in Tennessee during a specific time period, potentially constraining the applicability of its results to other educational contexts. For instance, the under representation of minority groups, such as Asian, Hispanic, and Native American students, in Project STAR could impact the accuracy of the intervention's effects on schools with different demographic compositions. 

### 5. Caveats

```{r, results='hide', warning = FALSE, message = FALSE, include = FALSE}
#Load packages
library(AER, warn.conflict = FALSE, quietly = TRUE)
library(dplyr, warn.conflict = FALSE, quietly = TRUE)
library(ggpubr,warn.conflict = FALSE, quietly = TRUE)
library(gplots,warn.conflict = FALSE, quietly = TRUE )
library(pracma, warn.conflict = FALSE, quietly = TRUE)
library(cowplot, warn.conflict = FALSE, quietly = TRUE)
library(car, warn.conflict = FALSE, quietly = TRUE)
library(RColorBrewer, warn.conflict = FALSE, quietly = TRUE)
library(knitr, warn.conflict = FALSE, quietly = TRUE)
library(DT, warn.conflict = FALSE, quietly = TRUE)
library(haven, warn.conflict = FALSE, quietly = TRUE)
library(cowplot, warn.conflict = FALSE, quietly = TRUE)
library(tidyr, warn.conflict = FALSE, quietly = TRUE)
library(knitr, warn.conflict = FALSE, quietly = TRUE)
library(ggplot2, warn.conflict = FALSE, quietly = TRUE)
```

```{r, warning = FALSE, message=FALSE, include = FALSE}
# Load file
star <- read_sav("dataverse_files/STAR_Students.sav")

# Select variables of interest
star2 <- subset(star, select = c(stdntid, gender, race ,flagg1, FLAGSG1, g1absent, g1classsize, g1classtype,
                                 g1freelunch, g1mathbsobjpct, g1mathbsobjraw, g1mathbsraw ,g1motivraw, g1present,
                                 g1promote, g1readbsobjpct,g1readbsobjraw, g1readbsraw, g1schid, g1selfconcraw, 
                                 g1speced, g1specin, g1surban, g1tcareer, g1tchid, g1tgen, g1thighdegree, g1tlistss,
                                 g1tmathss, g1trace, g1treadss, g1tyears, g1wordskillss))
# create a variable for school district
star2$district <- substr(star2$g1schid, 1, 3)
```

```{r, include = FALSE}
# Remove rows where the class type is NA and the math score is NA (this will be used for complete cases analysis)
star_na <- star2[!is.na(star2$g1classtype) & !is.na(star2$g1tmathss) ,  ]
```

```{r, include = FALSE}
# Get students who are in star in grade 1
in_star = star2[star2$FLAGSG1 ==1 , ]
in_star2 = subset(in_star, select = c(stdntid, race ,flagg1, FLAGSG1, g1classsize, g1classtype,
                                      g1freelunch, g1schid, g1surban,g1tmathss))
# Get students who are not in star in grade 1
not_in = star2[star2$FLAGSG1 ==0 , ]
```

#### 5.1 Missing Data

##### 5.1.1 Inspection

```{r,  include = FALSE}
common_column <- 'stdntid'
# Get all info for students who are not in STAR in grade 1
matched_data <- merge(star, not_in, by=common_column)
```

```{r, include = FALSE}
# Split based on number of years of participation in STAR
data_with_1 <- subset(matched_data, yearsstar == "1")
data_with_2 <- subset(matched_data, yearsstar == "2")
data_with_3 <- subset(matched_data, yearsstar == "3")
data_with_na <- subset(matched_data, is.na(yearsstar))
```

```{r, include = FALSE}
table(data_with_1$FLAGSGK, useNA="always")
table(data_with_1$FLAGSG2, useNA="always")
table(data_with_1$FLAGSG3, useNA="always")
# 3537/4772 students who are not in star in the first grade, only participate in the project for 1 year
# 1669 joined in kindergarten, then did not come back
# 585 joined in grade 2
# 1283 joined in grade 3
```

```{r, include= FALSE, message=FALSE, warning= FALSE}
# Studying those participating for 2 years
combinations_counts <- data_with_2 %>%
  group_by(FLAGSGK, FLAGSG1.x, FLAGSG2, FLAGSG3) %>%
  summarise(count = n())

# 1161 of the missing data students only in star for 2 years, majority (1094) joined from grade 2
# and the reminder joined in kindergarten then come back again in grade 2 or 3
# the remaining 74/4772 joined star for 3 years, but missing first grade. 
```

```{r, include = FALSE}
#Inspect students who doesn't return 
no_return <- subset(data_with_1, FLAGSGK == 1)
return <- star[star$FLAGSGK == 1 & star$FLAGSG1 == 1, ]
```

```{r, include= FALSE}
table(no_return$gkrepeat, useNA= "always")
table(return$gkrepeat, useNA= "always")
# 1599/1669 repeats kindergarten in the year 85-86
# exploring some variables that might relate to non-returning students such as teacher teaching experience, 
# after comparing teacher spec: race, gender, experience, no dramatic difference (similarly distributed)
# comparing class size average no return = 20.69, return 20.17 (no difference)
# gkpresent similarly distributed, altho average return: 159.2, average no return = 149.1
# since no specific pattern then, student didn't came back to the project star not for these reasons
```

```{r, include= FALSE}
# see the students who are missing score in math
in_star_1<- star[star$FLAGSG1 == 1, ]
in_star1_ns<- subset(in_star_1, is.na(g1tmathss))
# no particular pattern within class type, size, students' gender or race, or prior star involvement when trying to understand the remaining 231 missing values
# we can justify removing the missing values (or can conduct one testing, mcar?)
```

Among the 11,601 observations in the entire data set, we found that only 6,829 students engaged with STAR during Grade 1, while the remaining 4,772 did not partake in the program during Grade 1. This resulted in missing values across multiple grade-1 variables. Further analysis revealed that 3,537 of these students only participated in the project for 1 year, with varying entry points (1669 participated during kindergarten, 585 participated during the second grade, and 1283 participated during the 3rd grade), while 1,161 were in STAR for 2 years and 74 joined for 3 years, missing only the first grade.

We also observed that 36.4% of the students who joined Star in kindergarten either stopped participating in the program or came back again later for grade 2 or 3. However, a huge percentage, 96%, did not come back to star schools after kindergarten. We compared data between students who continued in Project Star for 2 consecutive years from kindergarten to grade 1 and those who did not. We inspected students’ data (gender, race, socioeconomic status), school data (urbanicity, class, size), teachers’ data (gender, race, and experience), as well as days school attended. All the numbers and data seemed to be distributed similarly. We observed average class size: no return = 20.69, return = 20.17, and although average days present at school kindergarten for those who return are 159.2 and no return = 149.1, they are still similarly distributed. Therefore, given the absence of any noticeable pattern, the reason students left STAR schools after kindergarten cannot be explained by the aforementioned variables. Hence, we can conclude that the 4772 missing data are missing at random, due to the fact that the students in the corresponding rows are not in STAR during grade 1.

Now let's focus our interest in studying the missing values for students who participated in grade1

```{r, fig.width = 5, fig.height = 4, fig.align='center', echo = FALSE}
# Calculate the count of missing values for each variable
missing_count <- colSums(is.na(in_star2))

# Create a data frame for plotting
missing_data <- data.frame(variable = names(missing_count), missing_count)

# Create a bar plot
ggplot(missing_data, aes(x = reorder(variable, -missing_count), y = missing_count)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Count of Missing Values in Variables of Interest",
       x = "Variables",
       y = "Count of Missing Values") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The bar chart depicting missing values across grade 1-related variables prompts an examination into underlying causes for some missigness in variables of interest. First, the g1tmathss variable, which denotes scaled math scores for grade 1, we observed 231 remaining missing values. From the plot, we can also see that the variable "g1tchid" exhibits no missing values. This suggests a potential avenue for further investigation. Delving into missingness patterns based on teacher, reveals a correlation with district delineation. Districts 244 and 168 emerge as significant contributors, collectively accounting for 43% of the 231 missing math values, as well as a substantial portion of missing race (69%) information.  District 244 also contributes to 43%  of missing values in free-lunch information along with district 203, 221. Districts 168, 203, and 221 are classified as rural schools, while District 224 covered both inner-cities and suburban schools, with majority located in inner-city. Given the observed pattern of missing values, with certain districts disproportionately contributing to missing data across critical grade 1 variables such as math scores, race information, and free-lunch status, we will resort to imputation as a solution to address this data incompleteness.

```{r, include = FALSE, message=FALSE}
# Count how many missing scores in math per teacher
missing_count_per_teacher <- in_star %>%
  group_by(g1tchid, district) %>%
  summarise(missing_count = sum(is.na(g1tmathss)))%>%
  arrange(desc(missing_count))

sum_missing_count_by_district <- missing_count_per_teacher %>%
  group_by(district) %>%
  summarise(sum_missing_count = sum(missing_count, na.rm = TRUE))
# District 244 (66), 168(33) has the most missing values (231)
```

```{r, include = FALSE, message=FALSE}
# Count how many missing value for race based on teacher
missing_count_race<- in_star %>%
  group_by(g1tchid, district) %>%
  summarise(missing_count = sum(is.na(race)))%>%
  arrange(desc(missing_count))

sum_missing_count_by_race <- missing_count_race %>%
  group_by(district) %>%
  summarise(sum_missing_count = sum(missing_count, na.rm = TRUE))
# District 244 (8), 168(11) has the most missing values (29)
```

```{r , include = FALSE, message=FALSE}
# Check how many missing values for free lunch category based on teacher
missing_count_fl<- in_star %>%
  group_by(g1tchid, district) %>%
  summarise(missing_count = sum(is.na(g1freelunch)))%>%
  arrange(desc(missing_count))

sum_missing_count_fl <- missing_count_fl %>%
  group_by(district) %>%
  summarise(sum_missing_count = sum(missing_count, na.rm = TRUE))
# District 244, 203 (26), 221 (24) , has the most missing values (179)
```

```{r, include = FALSE}
# Check how many missing scores per teacher
missing_count_per_teacher <- in_star %>%
  group_by(g1tchid, district) %>%
  summarise(missing_count = sum(is.na(g1tmathss)))%>%
  arrange(desc(missing_count))

sum_missing_count_by_district <- missing_count_per_teacher %>%
  group_by(district) %>%
  summarise(sum_missing_count = sum(missing_count, na.rm = TRUE))
```

##### 5.1.2 Imputation Process

1. Math score imputation 

The method that we are going to use to impute the missing math scores is by using the average of each class. This method preserves the characteristic performance level of each class, reflecting underlying disparities in teaching quality or student demographics. Using class averages simplifies interpretation for stakeholders and minimizes bias compared to more complex imputation techniques. Moreover, it mirrors real-world educational practices and leverages readily available data within the dataset, ensuring completeness without relying on external sources. Additionally, class average imputation is robust to small sample sizes within each class, making it a pragmatic choice for handling missing data in this context. 

We performed mean imputation by first calculating the mean of math scores for each classroom. This involved aggregating the math scores of all students within each classroom and computing the average. For observations with missing math scores, we replaced them with the mean value corresponding to their respective classroom, identified by matching the teacher ID. Essentially, this approach involved substituting missing values with the average math score of students in the same classroom, ensuring consistency with the learning environment and instructional context. 

```{r, include = FALSE}
# Get the average of math scores based on teacher ID
score_data <- in_star %>%
  group_by(g1tchid) %>%
  summarise(average_g1tmathss = mean(g1tmathss, na.rm = TRUE))
```

```{r, include = FALSE}
# Replace NA values in g1tmathss with the corresponding average score
imp_data <- in_star %>%
  left_join(score_data, by = "g1tchid")

imp_data <- imp_data %>%
  mutate(g1tmathss = ifelse(is.na(g1tmathss), average_g1tmathss, g1tmathss)) %>%
  select(-average_g1tmathss) 
```

2. Race imputation 

Imputing missing race data using the mode of each class involves leveraging the most frequently occurring race category within each distinct class or educational grouping. This method entails identifying different classes through teacher ID, and calculating the mode of the race category, for each class. Once the modes are determined, missing race values within each class are then replaced with the corresponding mode for that particular class. Utilizing class-specific modes acknowledges contextual relevance, reflecting distinct racial compositions across classes or cohorts. This approach simplifies imputation by leveraging available dataset information efficiently and practically addresses missing race data.

```{r, include = FALSE}
# Setting variable type to character
in_star$race <- as.character(in_star$race)
# Get race mode for each class
race_data <- in_star %>%
  group_by(g1tchid) %>%
  summarise(mod_rc = as.factor(names(sort(table(race), decreasing = TRUE))[1]))
```

```{r, include = FALSE}
# Replace NA values in race with the corresponding predicted race
imp_data <- imp_data %>%
  left_join(race_data, by = "g1tchid")
imp_data <- imp_data %>%
  mutate(race = ifelse(is.na(race), mod_rc, race)) %>%
  select(-mod_rc)
```

3. Free lunch imputation 

Imputing missing free lunch information using the mode of each school involves utilizing the most frequently occurring free lunch status within each distinct school. This approach is adopted due to the presence of two classrooms with entirely missing free-lunch information, specifically for teacher IDs 20345206 and 22157106, which both represent regular and aide classes with 24 students each situated in a rural location. Since individual classroom-level free lunch data is unavailable for these classrooms, aggregating the free lunch status at the school level provides a feasible alternative. By calculating the mode, or the most common free lunch status, for each school, missing free lunch values within each class was imputed with the corresponding mode for that particular school.

```{r,include = FALSE}
# Setting variable type to character
in_star$g1freelunch <- as.character(in_star$g1freelunch)
# Calculate mode of free lunch status for each class
fl_data <- in_star %>%
  group_by(g1schid) %>%
  summarise(mod_fl = as.factor(names(sort(table(g1freelunch), decreasing = TRUE))[1]))
```

```{r, include = FALSE}
missing_g1freelunch_rows <- imp_data[is.na(in_star$g1freelunch), ]
```

```{r, include = FALSE}
# Replace NA values in race with the corresponding predicted free lunch variable
imp_data <- imp_data %>%
  left_join(fl_data, by = "g1schid")
imp_data <- imp_data %>%
  mutate(g1freelunch = ifelse(is.na(g1freelunch), mod_fl, g1freelunch)) %>%
  select(-mod_fl)
```

#### 5.2 Heterogeneity

```{r, include = FALSE}
## NEXT WE INSPECT HETEROGENEITY
# We want to focus on the following grouping in the data: gender, race, urbanicity, free_lunch
# GENDER
table(in_star$gender, useNA = "always")
# Output seems approximately proportional between male and female
```

```{r, include = FALSE}
# Separate based on gender
male <- in_star[in_star$gender == 1 & complete.cases(in_star$gender), ]
female <- in_star[in_star$gender == 2 & complete.cases(in_star$gender), ]
```

```{r,include = FALSE}
# RACE
table(in_star$race, useNA = "always")
# Output seems approximately proportional between male and female
```

```{r, include = FALSE}
# Splitting data based on race
white <- in_star[in_star$race == 1 & complete.cases(in_star$race), ]
black <- in_star[in_star$race == 2 & complete.cases(in_star$race), ]
asian <- in_star[in_star$race == 3 & complete.cases(in_star$race), ]
hispanic <- in_star[in_star$race == 4 & complete.cases(in_star$race), ]
native <- in_star[in_star$race == 5 & complete.cases(in_star$race), ]
other <- in_star[in_star$race == 6 & complete.cases(in_star$race), ]
```

```{r, include = FALSE}
# The score_na data contains race 1-6. We want to inspect if the program has a different impact on different race
# Due to lack of representation from certain race, labeling into majority and minority is the most reasonable grouping
in_star <- in_star %>%
  mutate(label = ifelse(race == 1, "majority", "minority"))
```

```{r, include = FALSE}
# Create new category for plotting
in_star <- mutate(in_star, race_category = case_when(
  race == 1 ~ "White",
  race == 2 ~ "Black",
  is.na(race) ~ NA_character_,
  TRUE ~ "Other"
))
```

```{r, include = FALSE}
major <- in_star[in_star$label == "majority" & complete.cases(in_star$race), ]
minor <- in_star[in_star$label == "minority" & complete.cases(in_star$race), ]
```

```{r, include = FALSE}
# URBANICITY
table(in_star$g1surban, useNA = "always")
```

```{r, include = FALSE}
# Splitting based on urbanicity type for further exploration
inner <- in_star[in_star$g1surban == 1 & complete.cases(in_star$g1surban), ]
suburb <- in_star[in_star$g1surban == 2 & complete.cases(in_star$g1surban), ]
rural <- in_star[in_star$g1surban == 3 & complete.cases(in_star$g1surban), ]
urban <- in_star[in_star$g1surban == 4 & complete.cases(in_star$g1surban), ]
```

```{r, include = FALSE}
# FREE-LUNCH
table(in_star$g1freelunch, useNA = "always")
```

```{r, include = FALSE}
# Splitting based on freelunch status for further exploration
free <- in_star[in_star$g1freelunch == 1 & complete.cases(in_star$g1freelunch), ]
nonfree <- in_star[in_star$g1freelunch == 2 & complete.cases(in_star$g1freelunch), ]
```

```{r, fig.height = 2, fig.width = 8, fig.align = "center", warning=FALSE, message=FALSE, echo=FALSE}
## bar plot showing school urbanicity count, with each bar being split based on race categories
p1 <- ggplot(in_star) + 
  geom_bar(aes(x = as.factor(g1surban), fill = factor(race_category)), width = 0.5) +  
  scale_x_discrete(labels = c("1" = "Inner-city", "2" = "Suburban", "3" = "Rural", "4" = "Urban")) +
  scale_fill_brewer(palette = "Set3") +
  labs(x = "School Urbanicity", title = "Student Race Distribution") +
  labs(fill = "Race") +
  theme(
    plot.title = element_text(size = 12, face = "bold"),  # Adjust title size
    axis.title.x = element_text(size = 10)  # Adjust x-axis label size
  )

## bar plot showing school urbanicity count, with each bar being split based on free-lunch categories
p2 <- ggplot(in_star) + 
  geom_bar(aes(x = as.factor(g1surban), fill = factor(g1freelunch)), width = 0.5) +
  scale_x_discrete(labels = c("1" = "Inner-city", "2" = "Suburban", "3" = "Rural", "4" = "Urban")) +
  scale_fill_brewer(palette = "Set4") +
  labs(x= "School Urbanicity", title = "Free-lunch distribution") +
  labs(fill = "Free lunch") + 
  theme(
    plot.title = element_text(size = 12, face = "bold"),  # Adjust title size
    axis.title.x = element_text(size = 10)  # Adjust x-axis label size
  )

# Adjust the height of the title in the combined plot
plot_grid(p1, p2, ncol=2)
```

In the context of heterogeneity analysis within Project STAR, the data effectively ensures a balanced gender distribution across various class types, encompassing small, regular, and regular + aide classes. However, it is imperative to recognize disparities in the representation of racial and ethnic groups. To have more understanding of diversity within each school based on urbanicity, let's look at the following figure. We have school urbanicity as our x-axis, and the number of students participating in the STAR program across various schools as the y-axis. Note that the majority of students are situated in rural schools (47.4%), and the bar plot is further segmented based on students' race. The analysis reveals that White students predominate in Suburban (61%), Rural (93%), and Urban (85%) schools, while Inner-city schools are predominantly attended by Black students (96%). Due to the limited representation of other races, their participation is challenging to depict accurately; however, students identifying as Asian, Hispanic, Native American, and Other are primarily distributed among Suburban, Rural, and Urban schools. Only three Asians enrolled in inner-city schools. We also observed that the underrepresentation of minorities is accompanied with some missing values in their math scores. Specifically, among the 22 Asians participating in the STAR project, only 19 observations have recorded math scores, while out of nine Native Americans, only four have recorded math scores.

When delving into the free-lunch distribution across school locations, proportions in suburban, rural, and urban schools exhibit no alarming discrepancies. However, in inner-city schools, a staggering 90% of students receive free lunch, with 95% of them being Black students. This underscores a potential high level of economic disadvantage within the inner-city school community, as free lunch programs often serve as indicators of socioeconomic status. Economic disparities can contribute to educational inequalities, impacting students' access to resources, academic support, and opportunities. These findings prompt us to furhter understand the relationship between math score and school urbanicity, race, and free-lunch status.

```{r, fig.height = 3, fig.width = 9, fig.align = 'center', echo = FALSE, warning= FALSE}
par(mfrow = c(1, 3))
# Plot showing the main effect of school urbanicity to the math score variable
plotmeans(g1tmathss~as.factor(g1surban),data=in_star,xlab="School Urbanicity",
          ylab="Average Math Score",
          main="Main  effect, School Urbanicity",
          cex.lab = 1.2,  # Increase label size
          col.main = "blue",  # Set main title color
          col.lab = "darkgreen",  # Set axis label color
          col.axis = "darkgreen",  # Set axis tick color
          col = "royalblue",
          cex.lab=1.0)
grid()
axis(side = 1, at = 1:4, labels = c("Inner-city", "Suburban", "Rural", "Urban"), col.axis = "darkgreen")

# Plot showing the main effect of race to the math score variable
plotmeans(g1tmathss~as.factor(race_category),data=in_star,xlab="School Urbanicity",
          ylab="Average Math Score",
          main="Main  effect, Race",
          cex.lab = 1.2,  # Increase label size
          col.main = "blue",  # Set main title color
          col.lab = "darkgreen",  # Set axis label color
          col.axis = "darkgreen",  # Set axis tick color
          col = "royalblue",
          cex.lab=1.0)
grid()

# Plot showing the main effect of free lunch to the math score variable
plotmeans(g1tmathss~as.factor(g1freelunch),data=in_star,xlab="School Urbanicity",
          ylab="Average Math Score",
          main="Main  effect, Free lunch",
          cex.lab = 1.2,  # Increase label size
          col.main = "blue",  # Set main title color
          col.lab = "darkgreen",  # Set axis label color
          col.axis = "darkgreen",  # Set axis tick color
          col = "royalblue",
          cex.lab=1.0)
grid()
axis(side = 1, at = 1:2, labels = c("Yes", "No"), col.axis = "darkgreen")
```

```{r, include = FALSE}
# checking for multicolinearity
lm_model <- lm(g1tmathss ~ as.factor(g1surban) + as.factor(race_category) + as.factor(g1freelunch), data = in_star)
# Calculate VIF values
vif_values <- vif(lm_model)
```

From the main effect plot above, we can see that there exist clear mean differences between groups within different school urbanicity, races, and free lunch. Namely, students who went to inner-city schools, students who identify as black, and those who are on free lunch appear to have lower means than their peers. Since school urbanicity seems to contain some relationship with free-lunch and race variables, we checked for multicolinearity. The tool that we used is the Variance Inflation Factor (VIF) which measures the severity of multicollinearity by examining how much the variance of the estimated regression coefficients is inflated due to multicollinearity. Note that the interpretation of this testing is as follows:

- VIF < 5: Indicates low multicollinearity
- 5 ≤ VIF < 10: Indicates moderate multicollinearity
- VIF ≥ 10: Indicates high multicollinearity

For each variable that we are testing, we get Generalized Variance Inflation Factor (GVIF) between 1 and 2.3, and normalized GVIF close to 1, which indicates low multicollinearity. 
Since the three variables did not have high level of multicollinearity, we can include school urbanicity, race, and free-lunch indicator variables in our model to account for diversity and socioeconomic discrepancy.

However, it is important to understand the relationship between school ID and school urbanicity before incorporating both terms into the same model. Within our dataset, each school possesses a unique school ID, and each school is categorized into one specific type of urbanicity. Consequently, the inclusion of both variables may introduce a dependency issue among the predictor variables.

To further clarify this, a chi squared test for independence is performed, where the hypotheses tested are as follows:

- $H_0:$ School ID is independent of School Urbanicity
- $H_a:$ School ID is not independent of School Urbanicity

```{r, include = FALSE}
#chisq.test(agg_data$g1schid, agg_data$g1surban)
```

The resulting p-value is found to be less than 2.2e-16, indicating a rejection of the null hypothesis and suggesting that school ID is indeed not independent of school urbanicity.

Thus, we should consider only using one variable at a time. In the primary analysis, school ID will be employed, while school urbanicity will be reserved for sensitivity analysis.

#### 5.3 Noncompliance

```{r, include = FALSE}
# Aggregate Data based on teacher ID, and use mean as a summary stat of math score for each class
# To get each observation at teacher level
agg_data <- imp_data %>%
  group_by(g1tchid, g1schid, g1surban, g1classtype, g1classsize) %>%
  summarise(avg_scores = round(mean(g1tmathss)))
```

```{r, include = FALSE}
# Subsetting aggregated data based on class type
class1 <- subset(agg_data, g1classtype == 1)
class2 <- subset(agg_data, g1classtype == 2)
class3 <- subset(agg_data, g1classtype == 3)
```

```{r, include = FALSE}
# Classify entries into groups
class1 <- class1 %>%
  mutate(classification = case_when(
    g1classsize >= 13 & g1classsize <= 17 ~ "13-17",
    g1classsize >= 18 & g1classsize <= 21 ~ "18-21",
    g1classsize >= 22 & g1classsize <= 25 ~ "22-25",
    g1classsize < 13 ~ "<13",
    TRUE ~ NA_character_
  ))

# Create a pie chart shwoing actual class size distribution in classes labeled as "small"
c1 <- ggplot(class1, aes(x = "", fill = classification)) +
  geom_bar(width = 1, color = "white") +
  coord_polar(theta = "y") +
  geom_text(
    aes(label = sprintf("%0.1f%%", 100 * after_stat(count) / sum(after_stat(count)))),
    position = position_stack(vjust = 0.5),
    stat = "count"
  ) +
  labs(title = "Class Size : Small Classes (124) ",
       fill = "Class Size Range")
```

```{r, include=FALSE}
# Classify entries into groups
class2 <- class2 %>%
  mutate(classification = case_when(
    g1classsize >= 13 & g1classsize <= 17 ~ "13-17",
    g1classsize >= 18 & g1classsize <= 21 ~ "18-21",
    g1classsize >= 22 & g1classsize <= 25 ~ "22-25",
    g1classsize < 13 ~ "<13",
    g1classsize > 25 ~ ">25",
    TRUE ~ NA_character_
  ))

# Create a pie chart shwoing actual class size distribution in classes labeled as "regular"
c2 <- ggplot(class2, aes(x = "", fill = classification)) +
  geom_bar(width = 1, color = "white") +
  coord_polar(theta = "y") +
  geom_text(
    aes(label = sprintf("%0.1f%%", 100 * after_stat(count) / sum(after_stat(count)))),
    position = position_stack(vjust = 0.5),
    stat = "count"
  ) +
  labs(title = "Class Size : Regular Classes (115) ",
       fill = "Class Size Range")

    
```

```{r, include = FALSE}
# Classify entries into groups
class3 <- class3 %>%
  mutate(classification = case_when(
    g1classsize >= 13 & g1classsize <= 17 ~ "13-17",
    g1classsize >= 18 & g1classsize <= 21 ~ "18-21",
    g1classsize >= 22 & g1classsize <= 25 ~ "22-25",
    g1classsize < 13 ~ "<13",
    g1classsize > 25 ~ ">25",
    TRUE ~ NA_character_
  ))

# Create a pie chart shwoing actual class size distribution in classes labeled as "Regular+aide"
c3 <- ggplot(class3, aes(x = "", fill = classification)) +
  geom_bar(width = 1, color = "white") +
  coord_polar(theta = "y") +
  geom_text(
    aes(label = sprintf("%0.1f%%", 100 * after_stat(count) / sum(after_stat(count)))),
    position = position_stack(vjust = 0.5),
    stat = "count"
  ) +
  labs(title = "Class Size : Regular + Aide Classes (100) ",
       fill = "Class Size Range")
```

The STAR project implemented three distinct class types: small classes (S) with 13-17 students, regular classes (R) with 22-25 students, and regular classes with a full-time teacher aide (RA) and 22-25 students. However, the dynamic nature of student mobility over the course of the four-year experiment significantly influenced the composition and size of STAR classes. Students transferring into STAR schools from non-STAR schools were randomly assigned to one of the class types, with the constraint that small classes could not exceed 17 students. On the other hand, students moving from one STAR school to another were placed in the same class type as they had participated in previously, provided space allowed. Despite these constraints, Project STAR encountered challenges in maintaining compliance with the designated class size regulations.

```{r, fig.width= 12, fig.height = 3, echo = FALSE, fig.align = 'center'}
combined_plot <- plot_grid(c1, c2, c3, ncol = 3); combined_plot
```

The data for grade 1 reveals that out of the available 339 classrooms, 124 were classified as small, 115 as regular, and 100 as regular with an aide. The first pie chart illustrates the class size distribution in classrooms labeled as "small," where almost 10% of the classes deviated from the predetermined range. Similarly, the second pie chart highlights the size distribution in classes labeled as "regular," with 40% of these classes not strictly adhering to STAR's sizing protocol. Notably, more than 30% of regular classes had fewer students, and less than 10% had more students than specified. The third chart, focusing on regular classes with an aide, also indicates noncompliance, with 18% of classes exceeding the range and around 27% falling below. This noncompliance is attributed to students moving out of a STAR school, leading to diminished class enrollments and occasionally causing regular classes to become as small as some of the designated small classes.

After further inspection on class size, the true count of each category look as follows:

```{r, echo = FALSE}
class_size_table <- data.frame(
  Class_size = c("<13", ">25", "13-17", "18-21", "22-25"),
  Count = c(2, 28, 114, 71, 124)
)
kable(class_size_table, align = "c", caption = "Summary of Class Size") 
#datatable(class_size_table, width = "50%", height = "200px",
#          options = list(
#           paging = FALSE, # Disable pagination
#            searching = FALSE, # Disable search box
#            ordering = FALSE ))
```

To further understand the relationship between true class size and average of math scores, let's look at the following scatterplot:

```{r, include = FALSE}
# Get mean score for each class type. This will be used in the following plot
mean_scores <- agg_data %>%
  group_by(g1classtype) %>%
  summarise(mean_avg_scores = mean(avg_scores, na.rm = TRUE))
```

```{r, fig.width = 5, fig.height = 4, fig.align = 'center', message = FALSE, echo = FALSE, warning = FALSE}
# Plot all math scores vs class size, with trend line and horizontal line indicating the means per class category
class_plot <- ggplot(agg_data, aes(x = g1classsize, y = avg_scores)) +
  geom_point(position = position_jitter(width = 0.2, height = 0.1), color = "black", alpha = 0.6) +  # Add jitter to x-axis only
  geom_smooth(method = "loess", se = FALSE, color = "royalblue", linetype = "dashed", size = 1) +  # Add a smooth curve (loess)
  labs(x = "Class Size", y = "Average Scores", title = "Class size vs average grade 1 math scores") + 
  theme_minimal() + 
  theme(plot.title = element_text(face = "bold", size = 13, color = "royalblue"),  # Bold and increase title font size
        axis.title = element_text(size = 12),  # Increase axis label font size
        axis.text = element_text(size = 10),  # Increase axis tick font size
        plot.margin = margin(1, 1, 1, 1, "cm")) +  # Adjust plot margins
  geom_segment(data = mean_scores[mean_scores$g1classtype == 1, ],
               aes(x = 13, xend = 17, y = mean_avg_scores, yend = mean_avg_scores),
               color = "red", linetype = "solid", size = 1) + 
  geom_segment(data = mean_scores[mean_scores$g1classtype == 2, ],
               aes(x = 22, xend = 25, y = mean_avg_scores, yend = mean_avg_scores),
               color = "orange", linetype = "solid", size = 1) + 
  geom_segment(data = mean_scores[mean_scores$g1classtype == 3, ],
               aes(x = 22, xend = 25, y = mean_avg_scores, yend = mean_avg_scores),
               color = "green", linetype = "solid", size = 1)

plot_grid(class_plot, ncol = 1)
```

In the plot above, we are plotting the average math scores against each class size.The blue curve was fitted using geom_smooth() using the locally weighted scatterplot smoothing method to fit the curve. LOESS fits a flexible regression model to the data by locally fitting a low-degree polynomial to subsets of the data. It captures the general trend in the data without assuming a specific parametric form. 

The plot has the following components:

- x-axis displaying the class size 
- y-axis displaying the average math score
- red dashed line : average of small class (based on assigned treatment)
- green dashed line : average of regular + aide class (based on assigned treatment)
- orange dashed line : average of regular class (based on assigned treatment)

In the depicted graph, a distinct pattern emerges as we traverse the x-axis. Initially, within the range spanning approximately 10 to 16, the blue curve remains largely flat and aligned with the average math score of small class, before transitioning into a decline until it reaches the 20 mark. This implies the existence of threshold for class size before students performance started decreasing. A slight bump is observed from 20 to 25 on the curve,with the peak aligning with mean of regular + aide class. This bump is suggesting minimal variation in math scores despite the increasing class size up until 25. Beyond 25, a subtle downturn is evident, indicating a modest decrease in scores with larger class sizes. Overall, this trend underscores a negative correlation between class size and average math scores, indicating a decline in student performance as class size increases. Notably, the average score maintains relative stability up to a certain threshold before witnessing a significant decline, followed by a gradual decrease as class size further increases. This nuanced relationship between class size and math scores highlights the intricate interplay of factors influencing student performance in varying classroom settings.

Given the potential discrepancy between assigned labels and actual class sizes, it's essential to account for the actual class size range to ensure accurate representation of each observation in our analysis. This adjustment enhances the validity of our conclusions by aligning our analysis more closely with the real-world conditions. As part of a sensitivity analysis, we will explore substituting the variable representing received treatment for the assigned variable to assess whether our conclusions remain consistent.

### 6. Descriptive Analysis

#### 6.1 Univariate Statistics

```{r, include = FALSE}
# Remove rows where the class type is NA and the math score is NA
star_na <- star2[!is.na(star2$g1classtype) & !is.na(star2$g1tmathss) ,  ]
```

##### 6.1.1 Scaled math score 

First, let's look at the summary statistics and distribution of the math scores

```{r, include =FALSE}
# Get summary statistics and create a summary table for the raw, imputed, and aggregated data
summary_table <- data.frame(
  Data = c("Raw data", "Imputed data", "Aggregated Data"),
  Min = c(min(star2$g1tmathss, na.rm = TRUE), min(imp_data$g1tmathss), min(agg_data$avg_scores, na.rm = TRUE)),
  Q1 = c(quantile(star2$g1tmathss, na.rm = TRUE, probs = 0.25), quantile(imp_data$g1tmathss, probs = 0.25), quantile(agg_data$avg_scores, na.rm = TRUE, probs = 0.25)),
  Median = c(median(star2$g1tmathss, na.rm = TRUE), median(imp_data$g1tmathss), median(agg_data$avg_scores, na.rm = TRUE)),
  Mean = c(round(mean(star2$g1tmathss, na.rm = TRUE)), round(mean(imp_data$g1tmathss)), 
           round(mean(agg_data$avg_scores, na.rm = TRUE))),
  Q3 = c(quantile(star2$g1tmathss, na.rm = TRUE, probs = 0.75), quantile(imp_data$g1tmathss, probs = 0.75), quantile(agg_data$avg_scores, na.rm = TRUE, probs = 0.75)),
  Max = c(max(star2$g1tmathss, na.rm = TRUE), max(imp_data$g1tmathss), max(agg_data$avg_scores, na.rm = TRUE)),
  Missing = c(sum(is.na(star2$g1tmathss)), sum(is.na(imp_data$g1tmathss)), sum(is.na(agg_data$avg_scores)))
)
```

```{r, echo = FALSE}
kable(summary_table, align = "c",caption = "Summary of Class Size") 
#datatable(summary_table, caption = 'Summary statistics of math scores for Raw vs Cleaned vs Aggregated')
```

The provided table showed the summary statistics of the scaled math scores for raw data, imputed data, and aggregated data. The aggregated data indicates the imputed data aggregated based on teacher ID. This is to get each observation at class level. Notice that the imputed data closely mirrors the statistics of the raw data, with the sole exception being the absence of missing values. This suggests that the imputation process successfully replaced missing values with plausible estimates without significantly altering the distribution or summary statistics of the variables. The aggregated data presents different statistics compared to both the raw and imputed data. This could indicate that the aggregation process itself introduced changes to the data distribution, possibly through combining or summarizing variables. 

To clarify this, let's look at the following density plot:

```{r, fig.width= 8, fig.height=2, echo = FALSE}
den_raw <- ggdensity(star_na$g1tmathss, main = "Density plot : Raw", xlab = "Scaled math scores")+
              theme_classic() + theme(legend.position = "none",
              plot.caption = element_text(size = 12)) + geom_vline(xintercept = mean(star_na$g1tmathss),
              linetype = "dashed", color = "red") + geom_vline(xintercept = median(star_na$g1tmathss), 
              linetype = "dashed", color = "blue") +
              labs(caption = "Mean: 531, Median: 529")

den_imp <- ggdensity(imp_data$g1tmathss, main = "Density plot : Imputed", xlab = "Scaled math scores") + 
            theme_classic() + theme(legend.position = "none",
            plot.caption = element_text(size = 12))+ geom_vline(xintercept = mean(imp_data$g1tmathss), 
            linetype = "dashed", color = "red") + geom_vline(xintercept = median(imp_data$g1tmathss), 
            linetype = "dashed", color = "blue") + 
            labs(caption = "Mean: 531, Median: 529 ")

den_agg <- ggdensity(agg_data$avg_scores, main = "Density plot : Aggregated", xlab = "Scaled math scores")+
               theme_classic() + theme(legend.position = "none",
               plot.caption = element_text(size = 12)) + geom_vline(xintercept = mean(agg_data$avg_scores),
               linetype = "dashed", color = "red") + geom_vline(xintercept = median(agg_data$avg_scores), 
               linetype = "dashed", color = "blue") +
               labs(caption = "Mean: 532, Median: 532 ")

plot_grid(den_raw, den_imp, den_agg, ncol = 3)
```

```{r, include = FALSE}
# Testing for normality of response variable
shapiro.test(agg_data$avg_scores)
```

(From left to right) We have the density of math scores for raw data, imputed data, and aggregated data. The red and blue dashed line corresponds to its mean and median respectively. Notice that the density plot of math scores across the raw, imputed, and aggregated data set reveals a roughly consistent shape, indicating minimal deviations throughout the transformation process. Thus we can infer that the imputation and aggregation process did not cause drastic alterations to the data. 

##### 6.1.2 Class Type 

Next, let's explore the different class types in our data

```{r, fig.width=11, fig.height=4, fig.align = 'center', echo = FALSE}
par(mfrow = c(1,3))

# Raw Data
class_freq <- table(star2$g1classtype, useNA = "always")
categories <- c("Small", "Regular", "Regular + aide", "NA" )
values <- c(2584,1925,2320,4772)
percentage <- round((values/sum(values))*100)
# Create a pie chart
pie(values, labels = paste(categories, "\n", values, "(", percentage, "% )"), 
    main = "Pie Chart: Class Type (Raw)", col = brewer.pal(length(categories), "Set1"))

# Imputed data
class_freq <- table(imp_data$g1classtype, useNA = "always")
categories <- c("Small", "Regular", "Regular + aide" )
values <- c(1925, 2584, 2320  )
percentage <- round((values/sum(values))*100)
# Create a pie chart
pie(values, labels = paste(categories, "\n", values, "(", percentage, "% )"), 
    main = "Pie Chart: Class Type (Imputed)", col = brewer.pal(length(categories), "Set1"))

#Aggregated Data
class_freq <- table(agg_data$g1classtype, useNA = "always")
categories <- c("Small", "Regular", "Regular + aide" )
values <- c( 124 , 115 , 100 )
percentage <- round((values/sum(values))*100)
# Create a pie chart
pie(values, labels = paste(categories, "\n", values, "(", percentage, "% )"), 
    main = "Pie Chart: Class Type (Aggregated)", col = brewer.pal(length(categories), "Set1"))
```

We have three different class types that exists in the data set, namely small, regular, and regular + aide. From a total of 11601 observations, we have 4772 NA (or null) entry for this variable, which is around 41% of the data, as displayed in the first figure above. The proportion of data for each class type is seem to be approximately equal, where we have 22% small class, 17% regular, and 20% regular + aide. After imputation of the data, the count of each class type should not change as we did not perform any imputation to this variable. We can clarify this through the second pie chart which displays the class distributions of the imputed data. Lastly, the the third pie chart displays the class distribution after aggregating the data based on teacher ID. Observe that imputation and data aggregation did not change much of the class distribution. They still roughly have similar proportion. 

##### 6.1.3 School ID

There are total 76 schools participating in grade 1 STAR project, let's inspect the participation of each school

```{r, include = FALSE}
unique_school_ids <- unique(agg_data$g1schid)

# Create a mapping between school IDs and index values
school_id_mapping <- data.frame(
  g1schid = unique_school_ids,
  index = seq_along(unique_school_ids)
)

# Merge the mapping with agg_data to add the index column
agg_data <- merge(agg_data, school_id_mapping, by = "g1schid", all.x = TRUE)

```

```{r, include = FALSE}
frequency_sch <- agg_data %>%
  group_by(index) %>%
  summarise(frequency = n())

# Identify the mode (most frequent value)
mode <- frequency_sch$index[which.max(frequency_sch$frequency)]

```

```{r, fig.width = 5, fig.height = 4, fig.align ='center',echo = FALSE}
# Create a bar plot to visualize the frequency
schid_plot <- ggplot(frequency_sch, aes(x = factor(index), y = frequency)) +
  geom_bar(stat = "identity", fill = "#ADD8E6", width = 0.7) +  # Remove color aesthetic
  labs(x = "School ID", y = "Frequency", title = "Frequency of School IDs") +
  geom_text(aes(label = frequency), vjust = -0.3, size = 3, color = "black") +  # Add text color
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10),  # Adjust x-axis label rotation and font size
        axis.text.y = element_text(size = 10),  # Adjust y-axis label font size
        plot.title = element_text(face = "bold", size = 14),  # Bold and increase title font size
        panel.grid.major = element_blank(),  # Remove major gridlines
        #panel.grid.minor = element_blank(),  # Remove minor gridlines
        axis.line = element_line(color = "black"),  # Add black axis lines
        axis.ticks = element_line(color = "black"))  # Add black axis ticks

plot_grid(schid_plot, ncol = 1)
#ADD8E6
```


From the bar chart, we can observe the distribution of School IDs participating in the Project STAR dataset. The chart has been relabeled to index 1-76 for better visualization (the school ID which consists of 6 digits and might overcrowd the graph). Each bar represents a unique School ID, and its height indicates the frequency of occurrence of that School ID in the data set. Notably, each school has at least three classes, which aligns with the STAR policy requiring each school to have at least one of each class type. Across the dataset, there are a total of 339 classrooms contributed by 22 schools with 3 classrooms, 25 schools with 4 classrooms, 13 schools with 6 classrooms, 7 schools with 4 classrooms, and 1 school with 12 classrooms. The school with the highest participation is school 169229, contributing four classes of each type. While participation across schools may not be equal, the inclusion of at least one class for each type in every school ensures a fair comparison between class types. This uniform representation mitigates potential biases arising from unequal class distribution, enhancing the validity of analyses regarding the impact of class size on academic outcomes.


##### 6.1.4 School Urbanicity

Let's look at the different types of urbanicity enclosed in our dataset

```{r, fig.width=11, fig.height=4, fig.align = 'center', echo = FALSE}
par(mfrow = c(1,3))

# Raw Data
school_freq <- table(star2$g1surban, useNA = "always")
sch_categories <- c("Inner City", "Suburban", "Rural", "Urban", "NA" )
sch_values <- c(1380,1586,3237,626,4772)
sch_percentage <- round((sch_values/sum(sch_values))*100)
# Create a pie chart
pie(sch_values, labels = paste(sch_categories, "\n", sch_values, "(", sch_percentage, "% )"), 
    main = "Pie Chart: School Urbanity (Raw)", col = brewer.pal(length(sch_categories), "Set3"))

#Imputed data
school_freq <- table(star_na$g1surban, useNA = "always")
sch_categories <- c("Inner City", "Suburban", "Rural", "Urban")
sch_values <- c(1380, 1586, 3237 , 626 )
sch_percentage <- round((sch_values/sum(sch_values))*100)
# Create a pie chart
pie(sch_values, labels = paste(sch_categories, "\n", sch_values, "(", sch_percentage, "% )"), 
    main = "Pie Chart: School Urbanity (Imputed)", col = brewer.pal(length(sch_categories), "Set3"))

#Aggregated Data
school_freq <- table(agg_data$g1surban, useNA = "always")
sch_categories <- c("Inner City", "Suburban", "Rural", "Urban")
sch_values <- c(70, 78,160,31)
sch_percentage <- round((sch_values/sum(sch_values))*100)
# Create a pie chart
pie(sch_values, labels = paste(sch_categories, "\n", sch_values, "(", sch_percentage, "% )"), 
    main = "Pie Chart: School Urbanity (Aggregated)", col = brewer.pal(length(sch_categories), "Set3"))

```

Due to interest in including school urbanicty in our model, we want also want to inspect different school types based on its urbanity, which are: inner city, suburban, rural, and urban. Similar to the previous variable, we have 4772/11601 (41%) missing information in this variable, as shown in the first pie chart. The proportion of different school categories that exist in this data set also shows some discrepancies, where 28% of the data is from rural schools, 14% suburban, 12% inner city, and only 5% from urban schools. The second pie chart displays the school urbanicity distributions after imputing the data, and the third pie chart displays the school urbanicity  distribution after aggregating the data based on teacher ID. Similarly, imputation and data aggregation did not change much of the school urbanicity distribution. They still roughly have similar proportion.

##### 6.1.5 Race

```{r, include = FALSE}
race_2 <- imp_data %>%
  filter(race == 2) %>%
  group_by(g1tchid) %>%
  summarise(count_2 = n())

no_race_2 <- data.frame(g1tchid = agg_data$g1tchid[!(agg_data$g1tchid %in% race_2$g1tchid)], 
                        count_2 = rep(0, 109)) 

combined_df <- bind_rows(race_2, no_race_2)

# Count total observations for each teacher ID
total_obs <- imp_data %>%
  group_by(g1tchid) %>%
  summarise(total = n())

# Merge the two datasets to ensure all teacher IDs are retained
freq_race <- left_join(combined_df, total_obs, by = "g1tchid")

# Calculate proportion of Black students in each classroom
freq_race <- freq_race %>% mutate(proportion_2 = count_2 / total)
```

```{r, include = FALSE}
agg_data <- left_join(agg_data, freq_race[, c("g1tchid", "proportion_2")], by = "g1tchid")
```

```{r, include= FALSE}
rc_dist <- data.frame(
  race = c(1, 2, 3, NA, 1, 2, 3, NA, 1, 2, 3, NA),
  variable = c("count_STAR", "count_STAR", "count_STAR", "count_STAR", 
               "count_STAR_Grade1", "count_STAR_Grade1", "count_STAR_Grade1", "count_STAR_Grade1",
               "count_Imputed", "count_Imputed", "count_Imputed", "count_Imputed"),
  value = c(7200, 4180, 87, 134, 4528, 2221, 51, 29, 4550, 2228, 51, 0)
)

variable_order <- c("count_STAR", "count_STAR_Grade1", "count_Imputed")

# Convert the variable column to a factor with the specified order
rc_dist$variable <- factor(rc_dist$variable, levels = variable_order)
```

```{r, include = FALSE}
rc_dist <- rc_dist %>%
  group_by(variable) %>%
  mutate(percentage = value / sum(value) * 100)
```

```{r, fig.height = 4, fig.width = 5, fig.align = "center", echo = FALSE}
rc_plot <- ggplot(rc_dist, aes(x = variable, y = value, fill = factor(race))) +
  geom_bar(stat = "identity", position = "stack", width = 0.5) +
  geom_text(aes(label = paste0(round(percentage), "%")), position = position_stack(vjust = 0.5), size = 3) +  # Add percentage labels
  labs(title = "Distribution of Race",
       x = "Variable",
       y = "Count",
       fill = "Race") +
  scale_x_discrete(labels = c("count_STAR" = "STAR", 
                              "count_STAR_Grade1" = "STAR Grade 1", 
                              "count_Imputed" = "Imputed data")) +
  scale_fill_manual(values = c("royalblue2", "plum1", "khaki1", "#58D68D"),
                     labels = c("1" = "White", "2" = "Black", "3" = "Others")) +  # Rename legend labels
  theme_minimal()

plot_grid(rc_plot, ncol = 1)
```

The grouped bar chart provides a visualization of the distribution of counts across different categories of race within each dataset, (from left to right) raw data set, data of students who participate in grade 1 star, and imputed data. From the chart, we can observe that the distribution patterns are similar between the datasets. 
The majority of students participating in the program are White and Black, with others including Asian, Hispanics, Native american, and other as observed above. This statistics is due to the location of the study which is in Tennesse in 1985, which might not contain a lot of population diversity. Now, paying attention at the proportion of the race, especially from STAR Grade 1 dataset to Imputed data, we did not see much shifts, which implies that the imputation method did not cause drastic alteration to the data. This consistency underscores the efficacy of the imputation process, ensuring robustness in the dataset for subsequent analyses.


##### 6.1.6 Free lunch

```{r, include = FALSE}
frequency_fl <- imp_data %>%
  group_by(g1tchid, g1freelunch) %>%
  summarise(count = n()) %>%
  spread(g1freelunch, count, fill = 0)
# Rename columns
colnames(frequency_fl) <- c("g1tchid", "count_1", "count_2")
frequency_fl$fl_prop <- frequency_fl$count_1 / (frequency_fl$count_1 + frequency_fl$count_2)
```

```{r, include = FALSE}
agg_data <- left_join(agg_data, frequency_fl[, c("g1tchid", "fl_prop")], by = "g1tchid")
```

```{r, fig.width = 8, fig.height = 4, fig.align='center', echo = FALSE}
par(mfrow = c(1,2))

custom_colors <- c("lightslateblue", "palegoldenrod", "lightskyblue2")
fl_instar <- table(in_star$g1freelunch, useNA = "always")
fl_instar_categories <- c("Eligible", "Not Eligible", "NA")
fl_values <- c(3429, 3221 , 179 )
fl_percentage <- round((fl_values/sum(fl_values))*100)
# Create a pie chart
pie(fl_values, labels = paste(fl_instar_categories, "\n", fl_values, "(", fl_percentage, "% )"), 
    main = "STAR G1: Free lunch", col = custom_colors, border = "white")

custom_colors2 <- c("lightslateblue", "palegoldenrod")
fl_instar2 <- table(imp_data$g1freelunch, useNA = "always")
fl_instar_categories2 <- c("Eligible", "Not Eligible")
fl_values2 <- c(3482, 3347  )
fl_percentage2 <- round((fl_values2/sum(fl_values2))*100)
# Create a pie chart
pie(fl_values2, labels = paste(fl_instar_categories2, "\n", fl_values2, "(", fl_percentage2, "% )"), 
    main = "Imputed Data: Free lunch", col = custom_colors2, border = "white")
```

The pie chart above provides a visualization of the distribution of free-lunch eligibility within each dataset, namely (from left to right) data of students who participate in grade 1 star, and imputed data. From the chart, we can observe that the distribution patterns are similar between the datasets. There seemed to be approximately equal proportion between students who are in the free-lunch program and those who were not. The data for grade 1 STAR has 179 missing information. After imputation process, we get the second pie chart, showing still approximately equal proportion between the 2 categories. This implies that the imputation method did not cause drastic alteration to the data. This consistency underscores the efficacy of the imputation process, ensuring robustness in the dataset for subsequent analyses.

#### 6.2 Multivariate Statistics

##### 6.2.1 Math Score and class type

```{r, include = FALSE}
# Split data per class type from original data without NA
small1 <- imp_data[imp_data$g1classtype ==1 , ]
reg1 <- imp_data[imp_data$g1classtype == 2,  ]
regaid1 <- imp_data[imp_data$g1classtype == 3 , ]
```

```{r, include = FALSE}
# Split data per class type from aggregated data
small2 <- agg_data[agg_data$g1classtype ==1 , ]
reg2 <-  agg_data[agg_data$g1classtype == 2,  ]
regaid2 <-  agg_data[agg_data$g1classtype == 3 , ]
```

```{r, echo = FALSE, fig.width = 5, fig.height = 4, fig.align='center'}
# Create boxplot of math scores, grouped by class type
srra_plot <- ggplot(agg_data, aes(x = as.factor(g1classtype), y = avg_scores, fill = as.factor(g1classtype))) + 
  geom_boxplot()+
  labs(title = "Comparison of Average Scaled Math Scores by Class Type", 
       x = "Class Types", y = "Math Scores",
       fill = "Class Types") +  
  scale_fill_manual(values = c("maroon", "darkseagreen3", "royalblue1"),
                    labels = c("1" = "Small", "2" = "Regular", "3" = "Regular + Aide")) +
  scale_x_discrete(labels = c("1" = "Small", "2" = "Regular", "3" = "Regular + Aide")) +
  theme_minimal() +  # Change to minimal theme for a cleaner appearance
  theme(axis.text.x = element_text(angle = 0, hjust = 1),  # Rotate x-axis labels for better readability
        legend.position = "top",  # Move legend to the top
        panel.grid.major = element_line(color = "gray80"),  # Add major grid lines
        panel.grid.minor = element_blank(),  # Remove minor grid lines
        legend.title = element_text(size = 12, face = "bold"),  # Adjust legend title appearance
        plot.title = element_text(size = 16, face = "bold"))  # Adjust plot title appearance

plot_grid(srra_plot, ncol = 1)
```

```{r, echo = FALSE}
summary_stats2 <- data.frame(
  Class_Type = c("Small", "Regular", "Regular + Aide"),
  Min = c(468.0, 473.0, 472.0),
  `1st Qu.` = c(520.8, 509.5, 514.5),
  Median = c(538.0, 524.0, 531.5),
  Mean = c(539.0, 525.4, 529.5),
  `3rd Qu.` = c(556.2, 541.0, 545.2),
  Max = c(610.0, 598.0, 582.0)
)
# Print the table using kable
kable(summary_stats2, align = "c")
```

The boxplot illustrates the distribution of average math scores among students grouped by class types: Small class, Regular class, and Regular + Aide class. Each boxplot displays the median, quartiles, and outliers of the data. The median scores reveal that students in the Small class exhibit slightly higher math performance (538) compared to students in the Regular (524) and Regular + Aide (531.5) classes. While the interquartile range (IQR) is widest for the Small class, indicating greater variability in scores, outliers are present in all three class types. Despite these variations, the differences in median scores between class types are relatively small, suggesting that class size alone may not be the sole determinant of math performance. Further investigation into additional factors such as teaching quality and student characteristics is warranted to better understand academic outcomes comprehensively.
  
##### 6.2.2 Math scores and school id

```{r, include = FALSE}
mean_scores_sch <- agg_data %>%
  group_by(g1schid) %>%
  summarise(mean_avg_scores = mean(avg_scores, na.rm = TRUE))

indexing <- 1:76
# Append the index to the mean_scores dataset
mean_scores_sch <- mean_scores_sch %>%
  mutate(index = indexing)
```

```{r, echo = FALSE, fig.width =5 , fig.height = 4, fig.align ='center'}
# Make scatterplot of school id vs average math score, with lines indicating the average of small
# regular and regular + aide class
schid_avg_plot <- ggplot(mean_scores_sch, aes(x = index, y = mean_avg_scores)) +
  geom_point(color = "black", size = 2, alpha = 0.7) +  # Increase the size of points for better visibility
  geom_hline(yintercept = 538.9758, linetype = "dashed", color = "red") +  # First horizontal line
  geom_hline(yintercept = 525.4348, linetype = "dashed", color = "blue") + # Second horizontal line
  geom_hline(yintercept = 529.52, linetype = "dashed", color = "green") +  # Third horizontal line
  geom_text(aes(x = 80, y = 538.9758, label = "S"), color = "red", hjust = 0, vjust = 1.5, size = 3.5) +  # Label for the first line
  geom_text(aes(x = 80, y = 525.4348, label = "R"), color = "blue", hjust = 0, vjust = -0.5, size = 3.5) +  # Label for the second line
  geom_text(aes(x = 80, y = 529.52, label = "RA"), color = "green", hjust = 0, vjust = -0.5, size = 3.5) +  # Label for the third line
  labs(x = "School ID Index", y = "Mean Average Scores",
       title = "Mean Average Scores by School ID Index") +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    panel.border = element_blank(),  # Remove plot border
    text = element_text(size = 11),  # Adjust text size
    plot.title = element_text(face = "bold"),  # Make title bold
    axis.title = element_text(face = "bold")  # Make axis labels bold
  )

plot_grid(schid_avg_plot, ncol = 1)
```

The above scatterplot showed the mean average scores for each school ID. The x-axis represents the school ID index,
ranging from 1 to 76 to represent each school, while the y-axis represents the corresponding to average scores of each school. The plot includes three dashed horizontal lines, representing the mean of each class type. The red line labeled "S" indicates the mean of small classes, the blue line labeled "R" indicates the mean of regular sized classes, and similarly, the green line indicates the mean of regular + aide classes. There are noticeable variations in the mean math scores among schools. Certain schools exhibit average scaled math scores below the regular class average, suggesting that the scores across all class types in these schools are likely to be lower than the average of regular-sized classes. Conversely, some schools demonstrate scores well above the mean of small classes, indicating that the scores of each class type in those schools surpass the average of small-sized classes.
Hence, we can infer that school ID may indeed capture some underlying differences in math scores across different schools.

### 7. Model Proposal

Before proposing a model, let's consider whether the response variable, i.e. math scores is normally distributed. Performing a Shapiro-Wilk normality test, we are testing $H_0 :$ data set is normally distributed, against $H_a :$ data set is not normally distributed. Setting the $\alpha = 0.05$ we get a p-value of 0.2046. Hence, we cannot reject the null hypothesis. Although failing to reject the null hypothesis does not necessarily mean that the data is normal, considering the density plot and the closeness of mean and median value we observed in the previous section, we can say that it is likely the math score variable is or approximately normally distributed. 

```{r, include = FALSE}
# Shapiro Wilk normality test (to check whether the response variable is normally dist)
shapiro.test(agg_data$avg_scores)
```

In this model, we will use average scaled math scores as our dependent variable. Note that the dataset that we will be using is an aggregated data set where each observation represents a classroom (this is achieved through grouping by teacher ID). Aggregating the dataset from each student to each classroom provides a more comprehensive and interpretable perspective on the relationship between class type and math scores, which is our primary goal. We are going to use class type and school ID as our predictor. Additionally, to account for race and SES disparity explained in heterogeneity section, we will add race and free-lunch indicator in our model. However, since we are dealing with aggregated data, we will resort to using the proportion of students who are on free-lunch for free-lunch indicator, and proportion of black student as a race indicator in our model. This choice of race is reasonable since the STAR project only contain 2 overwhelmingly dominant race i.e. black and white. Hence, with normally distributed response variable, 2 categorical variables and 2 continuous variable as predictors, we will consider a multiple linear regression model: 

$Y_{ijk} = \mu.. + \alpha_{i} + \beta_{j} + \gamma_1x_1 + \gamma_2x_2 + \epsilon_{ijk}$

Where:

- $Y_{ijk}$ represents the math score of kth observation for class type i,  and school id j
- $\mu..$ the average value of the dependent variable when all other factors and covariates are held constant at their reference levels
- $\alpha_{i}$ represents the effect of factor i (class type) on the dependent variable. Note that Each $\alpha_i$ coefficient represents the average change in the dependent variable when moving from the reference level to class type 2 and 3
- $\beta_{j}$ represents the effect of factor j (school id) on the dependent variable. Note that Each $\beta_j$ coefficient represents the average change in the dependent variable when moving from the reference level to level j
- $\gamma_1x_1$ represent the effects of continuous independent variable x1 (race proportion) on math score. 
- $\gamma_2x_2$ represent the effects of continuous independent variable x2 (free lunch proportion) on math score.

Assumptions of multiple linear regression:

- Homogeneity of variance (homoscedasticity)
- Independence of observations
- Normality

### 8. Model Fitting

Based on model proposal above, fitting the model gives us the following result :

```{r, include = FALSE}
# Model fitting for multiple linear regression
n_model <-lm(avg_scores ~ as.factor(g1classtype) + as.factor(g1schid) + fl_prop + proportion_2, data=agg_data)
```

```{r, include = FALSE}
# Create a dataframe for beta coefficients
coeff <- as.data.frame(n_model$coefficients)
schid_coeff <- as.data.frame(coeff[4:78,])
```

```{r, include = FALSE}
# Add corresponding school ID to the coefficient
g1schid_column <- mean_scores_sch$g1schid
g1schid_column <- g1schid_column[2:76]
schid_coeff$g1schid <- g1schid_column
```

$Y_{ijk} = 517 + \alpha_{i} + \beta_{j} -10.3 x_1 -15.9x_2$

Where:

- $\alpha_1 = 0$ , $\alpha_2 = -12.9$, $\alpha_3 = -11$
- $\gamma_1 = -10.3$
- $\gamma_2 = -15.9$
- $\beta_j$ as follows: 

```{r, echo = FALSE}
# print table using datatable
datatable(
  schid_coeff[, c("g1schid", "coeff[4:78, ]")], 
  options = list(pageLength = 5),
  colnames = c('ID', 'Coefficients'),
  caption = 'Coefficients of beta corresponding to each school id'
)
```

Explanation:

- Note that the reference level for $\alpha_i$ in this model is $\alpha_1$ which corresponds to the small class and the reference level for $\beta_i$ is school 112038
- The intercept of 517 represents the expected value of the response variable when both i and j are at their reference levels, and both $x_1, x_2$ are 0. Specifically, this implies that the mean of the scaled math score a small class in school 112038 where none of the student in the class is black, and none of the are on free lunch is 517.
- Notice that the other coefficients for $\alpha_i$'s are negative numbers. This implies, that within the same school and given the same proportion of students who identifies as black and are on free lunch, the average scaled math score is 12.9 points lower for regular class, and 11 points lower for regular + aide class compared to the small class. 
- On the other hand, the other coefficients for $\beta_i$'s are positive numbers. This implies that given the same class size and student race and SES proportion, students who attended schools other than 112038 will have a score that is 1.4 to 69 points higher. 
- The coefficient for $\gamma_1 = -10.25$. Note that the entry of $x_1$ values are proportion of students who are eligible for free-lunch (ranges from 0-1). This implies that holding everything else the same, as the proportion of students eligible for free lunch increases in a classroom, the average math score for that classroom decreases. Given a same type of classroom, a class where everyone receives free-lunch scores 10 points less compared to a classroom without students on free-lunch.
- The coefficient for $\gamma_2 = -15.96$. Note that the entry of $x_2$ values are proportion black student in a classroom (ranges from 0-1). This implies that holding everything else the same, as the proportion of black student increases in a classroom, the average math score for that classroom decreases. 

Moreover, we want to see wheter the addition of free-lunch and race variable is significant to the model

```{r, echo = FALSE}
# Display anova table
options(knitr.kable.NA = '')
sstable <- anova(n_model)
rownames(sstable) <- c("Class Type", "School ID", "Free lunch", "Race Proportion", "Residuals")
kable(sstable, format = "pandoc", digits = 3, caption = "ANOVA table") 
```

In the ANOVA table provided, there are several factors being evaluated: "Class Type," "School ID," "Free lunch," and "Race Proportion." Each factor is assessed for its contribution to explaining the variability in the dependent variable (Math score). The p-values associated with each factor indicate whether they are statistically significant predictors of the outcome variable. In this table, "Class Type," and "School ID" have p-values less than 0.05, indicating they are statistically significant predictors. On the other hand, "Race Proportion" and "Free lunch" has a p-value greater than 0.05, suggesting that given "Class Type," and "School ID" already in the model, adding "Race Proportion" and "Free lunch" did not contribute significantly to explaining the variability in the response variable.

To understand why the addition of race proportion and free lunch as predictors was insignificant in this model, we can reference the scatterplot discussed in section 6.2.2. From our observations, it became evident that school ID potentially encapsulates underlying distinctions in academic performance across schools. This implies that schools exhibiting consistently higher or lower average math scores may reflect various school-level factors impacting student achievement, including teaching quality, curriculum effectiveness, socioeconomic status, and potentially race demographics. Given that this information is likely to already get encapsulated within the school ID predictor, the addition of race proportion and free lunch variables might be redundant, hence their insignificance in improving the model's predictive power.

Let's consider dropping the free-lunch and race variable. A model with only class type and school ID as predictors yields this result, and let's compare this to the previous model with 4 predictors by looking at the anova table

```{r,  include = FALSE}
# Model fitting
n_model2 <-lm(avg_scores ~ as.factor(g1classtype) + as.factor(g1schid), data=agg_data)
```

```{r, echo = FALSE}
# Display anova table
options(knitr.kable.NA = '')
sstable <- anova(n_model2)
rownames(sstable) <- c("Class Type", "School ID", "Residuals")
kable(sstable, format = "pandoc", digits = 3, caption = "ANOVA table") 
```

The ANOVA table above illustrates the results of running the analysis with only class type and school ID as predictors. Both class type and school ID demonstrate significant contributions to explaining the variability in average scores, as indicated by their low p-values. Despite removing two predictors, the residuals for this model (72274) are only slightly higher than those of the previous model (71584). Additionally, the R-squared values for both models (0.672 and 0.676) are quite similar, suggesting comparable fits to the data in terms of explaining the response variable's variability. Given this similarity and our aim to prioritize simplicity and interpretability, we will opt for the simpler model. 

Model Fitting: (Model 1)

```{r, include= FALSE}
# Get coefficients of beta and create a dataframe
coeff2 <- as.data.frame(n_model2$coefficients)
schid_coeff2 <- as.data.frame(coeff2[4:78,])
schid_coeff2$g1schid <- g1schid_column
```

$Y_{ijk} = 502 + \alpha_{i} + \beta_{j}$, where:

- $\alpha_1 = 0$ , $\alpha_2 = -13.3$, $\alpha_3 = -11.4$
- $\beta_j$ as follows: 

```{r, echo = FALSE}
# Display beta coefficient using datatable
datatable(
  schid_coeff2[, c("g1schid", "coeff2[4:78, ]")], 
  options = list(pageLength = 5),
  colnames = c('ID', 'Coefficients'),
  caption = 'Coefficients of beta corresponding to each school id'
)
```

Model Explanation:

- Note that the reference level for $\alpha_i$ in this model is $\alpha_1$ which corresponds to the small class in variable "g1classtype" and the reference level for $\beta_i$ is school 112038
- The intercept of 502 represents the expected value of the response variable when both i and j are at their reference levels. Specifically, this implies that the mean of the scaled math score a small class in school 112038 is 502.
- Notice that the other coefficients for $\alpha_i$'s are negative numbers. This implies, that within the same school, the average scaled math score is 13.3 points lower for regular class, and 11.4 points lower for regular + aide class compared to the small class. 
- On the other hand, the other coefficients for $\beta_j$'s are mostly positive numbers, with some exceptions of 3 school IDs. The positive values implies that given the same class size, student who attend different school will score some points higher (according to its $\beta_j$ coefficient). On the other hand, negative values implies that student who attend different school will score some points lower (according to its $\beta_j$ coefficient). 

Notice that  The intercept changes from 517 in the first model to 502 in the second model. This change occurs because the intercept accounts for the constant value of the response variable when all predictor variables are zero. Removing the terms $−10.3x_1$ and $−15.9x_2$ effectively reduces the contribution of these terms to the intercept, resulting in the lower value of 502.

In the first model, $\beta_j$ is being adjusted by the values of $x_1, x_2$ (multiplied by -10.3 and -15.9 respectively). However, in the second model, these adjustments are removed, allowing the coefficient for $\beta_j$
to represent its direct effect on the response variable without the influence of $x_1, x_2$. This results in a higher coefficient for $\beta_j$ in the second model. 

With respect to our primary question, the negative coefficients for $\alpha_2$ and $\alpha_3$ indicate that, compared to the reference level (small class), the average scaled math scores are lower for both regular and regular + aide classes within the same school. Therefore, there are differences in the response variable based on the different levels of $\alpha_i$. Specifically, students in regular and regular + aide classes tend to have lower scaled math scores compared to students in small classes within the same school.

### 9. Model Diagnostic

In this section, we focus our attention on diagnosing two critical aspects of the statistical model: error normality and homogeneity. By evaluating these diagnostic measures, we aim to assess the assumptions underlying the model and ensure its reliability for inference.

#### 9.1 Homogeneity of variance (homoscedasticity)
  
```{r, echo = FALSE, fig.align='center'}
# Get residual vs fitted plot from model called n_model
plot(n_model2, which = 1, col = "royalblue", lty = 1, cex = 0.7, pch = 16)
```

The above plot shows residuals vs. fitted plot. Notice that the points scattered randomly around the horizontal line with no particular pattern. Since we see no cone-like or funnel-like shape (which indicates heteroscedasticity), it is likely that the variance of the residuals is constant across different values of the predictor variables. In other words, there should be no systematic relationship between the residuals and the fitted values.

To confirm this observation, we conducted a Breusch-Pagan Test, which examines whether heteroscedasticity is present in a regression model. The hypothesis in this test as follow:

- $H_0$ : The residuals are distributed with equal variance (Homoscedastic)
- $H_a$ : The residuals are not distributed with equal variance (Heteroscedastic)

Defining $\alpha = 0.05$, running this test returns a p-value of 0.0765. As this p-value exceeds the threshold, we fail to reject the null hypothesis, suggesting homoscedasticity in our data and hence, satisfying the homogeneity assumption.

```{r, include= FALSE}
bptest(n_model2)
```

#### 9.2 Normality

```{r, echo = FALSE, fig.height = 4, fig.width = 8, fig.align = 'center'}
# Get QQ plot and show histogram of residuals
par(mfrow=c(1,2))
plot(n_model2, which=2, col = "royalblue", lty = 1, cex = 1, col.axis = "darkgray",     
     col.lab = "darkgray")
hist(n_model2$residuals, 
     breaks = 30,               
     col = "royalblue",           
     border = "white",         
     main = "Histogram of Residuals",  
     xlab = "Residuals",        
     ylab = "Frequency",         
     xlim = c(min(n_model2$residuals), max(n_model2$residuals)),  
     las = 1,
     col.axis = "darkgray",     
     col.lab = "darkgray"
     )
```

When observing a normal QQ plot, we see the points align with the diagonal line in the middle but deviate towards the ends, it suggests that the residuals follow a normal distribution reasonably well in the central portion but deviate from normality in the tails. This pattern indicates that the bulk of the residuals are approximately normally distributed, which is a reassuring sign for the validity of assumptions underlying the regression model. However, the deviations observed towards the ends of the QQ plot suggest that the tails of the distribution may be heavier or lighter than those of a normal distribution. Visual inspection of the histogram reveals a roughly symmetrical distribution of the residuals, indicating a potential approximation to normality, although we did observe longer tail. To check the normality of the residuals, we ran a Shapiro Wilk test for normality since it is most effective for detecting deviations from normality in terms of symmetry and tail behavior. We have $H_0 :$ The data is normal, and $H_a :$ The data is not normal. Setting the $\alpha = 0.001$, the model returned p-value = 0.0001, which implies the rejection of null hypothesis, i.e. the residuals did not follow normal distribution. 

```{r, include = FALSE}
# Get lambda value from boxcox function
library(MASS)
LAMBDA <- MASS::boxcox(n_model)
LAMBDA$x[which(LAMBDA$y == max(LAMBDA$y))]
```

```{r, include = FALSE}
# Perform transformation to response variable
agg_data$log_score <- 1/(agg_data$avg_scores)^2
# Fit new model with new response
n_model_log <-lm(log_score ~ as.factor(g1classtype) + as.factor(g1schid), data=agg_data)
```

```{r, echo = FALSE, fig.height = 4, fig.width = 8, fig.align = 'center'}
# Get QQ plot and histogram of residuals
par(mfrow=c(1,2))
plot(n_model_log, which=2, col = "royalblue", lty = 1, cex = 1, col.axis = "darkgray",     
     col.lab = "darkgray")
hist(n_model_log$residuals, 
     breaks = 30,               
     col = "royalblue",           
     border = "white",         
     main = "Histogram of Residuals",  
     xlab = "Residuals",        
     ylab = "Frequency",         
     xlim = c(min(n_model_log$residuals), max(n_model_log$residuals)),  
     las = 1,
     col.axis = "darkgray",     
     col.lab = "darkgray"
     )
```

In an effort to address this challenge, we explored the option of transforming the data using the Box-Cox method. Upon examining the Box-Cox plot, we identified a $\lambda$ value of -1.79798, suggesting a transformation using the reciprocal of the square of y, or $\frac{1}{y^2}$ (transforming the average math score variable). Subsequently, this transformation resulted in a lambda value of 0.91, indicating that the data approximated a normal distribution, and the transformation worked as desired. When inspecting the Q-Q plot and histogram of the transformed model, we observed similar deviations from the reference line as previously encountered. Following the same procedure as before, (setting $\alpha = 0.001$) the Shapiro-Wilk test for normality yielded a p-value of 0.002478, indicating an inability to reject the null hypothesis and suggesting normality of the residuals.

Nevertheless, it is imperative to acknowledge that this transformation led to coefficients and fitted values approaching very small, positive or negative numbers close to zero (on the order of $10^{-6}$ or  $10^{-7}$). Given our primary objective, this presents a notable challenge to the interpretability of our results in addressing the research question of interest.

Considering the sensitivity of the Shapiro-Wilk test to sample size and the robustness of linear regression to violations of normality assumptions, it is reasonable to prioritize the interpretability of the untransformed model over minor deviations from normality. The Central Limit Theorem (CLT) supports the assumption of normality in residuals, especially with large sample sizes commonly encountered in linear regression analyses. While the Shapiro-Wilk test may detect slight deviations from normality, linear regression estimates remain unbiased and consistent, ensuring the validity of inferential procedures such as hypothesis testing and confidence intervals. Given the trade-off between interpretability and minor violations of normality, opting for the untransformed model maintains clarity and ease of interpretation without compromising the integrity of the analysis. Therefore, sticking with the untransformed model is justified, as the benefits of interpretability outweigh the minor deviations from normality.

### 10. Sensitivity Analysis

#### 10.1 Complete Cases Analysis

While imputation was performed in the main analysis, we understand that cautious consideration must be given before proceeding with data imputation methods, as imputed values may introduce biases if underlying patterns or mechanisms driving missingness are not fully understood and addressed. Hence, in this section, we will perform analyses using only complete cases (i.e., observations with no missing values) to assess the potential impact of missing data on study findings. Following our final model, we have:

Model Fitting: (Complete Case Model)

```{r, include = FALSE}
# Aggregate Data 
agg_data2 <- star_na %>%
  group_by(g1tchid, g1schid, g1surban, g1classtype, g1classsize) %>%
  summarise(avg_scores = round(mean(g1tmathss)))
```

```{r,  include = FALSE}
# fit model using complete case data
cc_model <-lm(avg_scores ~ as.factor(g1classtype) + as.factor(g1schid), data=agg_data2)
```

```{r, include = FALSE}
# get beta coefficients in dataframe
coeff_cc <- as.data.frame(cc_model$coefficients)
schid_coeff_cc <- as.data.frame(coeff_cc[4:78,])
schid_coeff_cc$g1schid <- g1schid_column
```

$Y_{ijk} = 502 + \alpha_{i} + \beta_{j}$, where:

- $\alpha_1 = 0$ , $\alpha_2 = -13.3$, $\alpha_3 = -11.4$
- $\beta_j$ as follows: 

```{r, echo = FALSE}
# display coefficient
datatable(
  schid_coeff_cc[, c("g1schid", "coeff_cc[4:78, ]")], 
  options = list(pageLength = 5),
  colnames = c('ID', 'Coefficients'),
  caption = 'Coefficients of beta corresponding to each school id'
)
```

Similar to the previous model:

- The model considers the reference level for $\alpha_i$ as $\alpha_1$ (small class) and for $\beta_i$ as school 112038.
- The intercept of 502 signifies the expected value of the response variable when both factors are at their reference levels, suggesting that the mean scaled math score for small classes in school 112038 is 502.
- Coefficients for $\alpha_i$ are negative, indicating lower average scaled math scores for regular and regular + aide classes within the same school compared to small classes.
- Most coefficients for $\beta_j$ are positive, implying higher scores for students attending different schools, with exceptions for 3 school IDs showing negative values, suggesting lower scores.

Notice that using the complete cases only yields the exact same result as using the imputed data. This is reasonable since we observed in the univariate statistics of the math scores that imputing the data did not alter the data. Thus, the result is still consistent with the previous finding.


#### 10.2 Heterogeneity: Replacing school ID with school urbanicity

The decision to replace school ID with school urbanicity in sensitivity analysis is justified by observed heterogeneity in math scores across different urbanicity categories, as discussed in the Caveats section. This suggests that urbanicity may significantly related to math scores, reflecting underlying differences in academic performance associated with different school environments. School urbanicity categorizes schools based on their geographic and population density characteristics. Accounting for this allows us some insights into how different types of communities may affect student outcomes. Additionally, this approach enhances the robustness of our findings and ensures that conclusions are not overly influenced by specific school identifications. 

First, let's fit the model: (Model 3)

```{r,  include = FALSE}
# fit model
model3 <-lm(avg_scores ~ as.factor(g1classtype) + as.factor(g1surban), data=agg_data)
```

```{r, include = FALSE}
# Get coefficients
coeff3 <- as.data.frame(model3$coefficients)
```

$Y_{ijk} =  517 + \alpha_{i} + \beta_{j}$, where:

- $\alpha_1 = 0$ , $\alpha_2 = -12.78$, $\alpha_3 = -9.85$
- $\beta_1 = 0$ , $\beta_2 = 24.46$, $\beta_3 = 29.03$, $\beta_4 = 25.24$

Explanation: 

- Note that the reference level for $\alpha_i$ in this model is $\alpha_1$ which corresponds to the small class in and the reference level for $\beta_i$ is $\beta_1$ which corresponds to the inner-city schools.
- The intercept of 517.19 represents the expected value of the response variable when both i and j are at their reference levels. Specifically, this means the mean of the scaled math score for grade 1 in small classes in inner-city schools is 517.19.
- Notice that the other coefficients for $\alpha_i$'s are negative numbers. This implies, that given the same school location, the average scaled math score is 12.78 points lower for regular class, and 9.85 lower for regular + aide class compared to the small class. 
- On the other hand, the other coefficients for $\beta_i$'s are positive numbers. This implies that given the same class size, students who went to suburban school will score on average 24.46 points higher, students in rural school will score 29.03 points higher, and students in urban schools will score 25.2 higher than those who went to schools located in inner-city.
- Based on this, we have learned that regardless of class sizes, students who went to inner-city schools generally have lower performance than their peers who went to different school location. Furthermore, noting that the value of $\beta - \alpha$ is always positive for all combination, this implies that the performance of students who are in small classes in inner city schools are lower than their peers who are in regular classroom in different school location. Thus, while star project was able to show that there exist an increase in mathematics performance, similar percentage of increase in math score does not imply an equal outcome in academic performance. 
- The data underscores a concerning trend, indicating that inner-city schools face additional challenges due to socioeconomic conditions. This revelation prompts a critical examination of the existing disparities and  call for more support for inner-city schools extends beyond the mere provision of resources; it advocates for a comprehensive approach that addresses the unique challenges faced by these institutions.

In relation to question of interest, despite of the differences in intercept term and $\alpha_i$, $\beta_j$ coefficients, we still observe the same pattern , that is negative coefficients for $\alpha_2$ and $\alpha_3$. Thus, students in regular and regular + aide classes tend to have lower scaled math scores compared to students in small classes within the same school, which is consistent with our previous finding.

#### 10.3 Noncompliance: Replacing class type with class size

While the use of assigned treatment label, class type, can provide a more interpretable analysis framework and facilitate direct comparison with existing literature, the deviation problem between class type and its actual size observed before prompt us to consider substituting the class type variable with the actual class sizing range as many classes . This adjustment reflects real-world conditions where classes can deviate from predetermined sizes due to various factors. By using the actual class sizing range instead of broad class type categories, the model can capture finer variations in class sizes, potentially improving its predictive performance and minimizing misclassification errors. Incorporating the actual class sizing range offers increased flexibility in exploring the relationship between class size and math scores, leading to a more nuanced understanding of the treatment effect.

For this, we are grouping the true class sizing as follows: "12-17", "18-21", 22-25", ">25".
This grouping is based on the plot in section 6.2.2, and consideration of numbers of observation for each class size

```{r, include= FALSE}
# Create new grouping in the dataset based on its true class size
agg_data <- agg_data %>%
  mutate(classification2 = case_when(
    g1classsize >= 12 & g1classsize <= 17 ~ "12-17",
    g1classsize >= 18 & g1classsize <= 21 ~ "18-21",
    g1classsize >= 22 & g1classsize <= 25 ~ "22-25",
    g1classsize > 25 ~ ">25",
    TRUE ~ NA_character_
  ))
agg_data$classification2 <- as.factor(agg_data$classification2)
```

```{r,  include = FALSE}
# set categorical data as factor
agg_data$classification2 <- as.factor(agg_data$classification2)
# Ensuring 12-17 is the reference level
agg_data$classification2  <- factor(agg_data$classification2, 
                            levels = c("12-17", "18-21", "22-25", ">25"))
#fit model
model4 <-lm(avg_scores ~ as.factor(agg_data$classification2) + as.factor(g1schid), data=agg_data)
```

```{r, include = FALSE}
# Get coefficient of beta
coeff4 <- as.data.frame(model4$coefficients)
schid_coeff4 <- as.data.frame(coeff4[5:79,])
schid_coeff4$g1schid <- g1schid_column
```

Fitting model: (Model 4)

$Y_{ijk} = 505.8 + \alpha_{i} + \beta_{j}$, where:

- $\alpha_1 = 0$ , $\alpha_2 = -11.6$, $\alpha_3 = -12.24$, $\alpha_4 = -17.15$
- $\beta_j$ as follows: 

```{r, echo= FALSE}
# display coefficient of beta
datatable(
  schid_coeff4[, c("g1schid", "coeff4[5:79, ]")], 
  options = list(pageLength = 5),
  colnames = c('ID', 'Coefficients'),
  caption = 'Coefficients of beta corresponding to each school id'
)
```

Explanation:

- Note that the reference level for $\alpha_i$ in this model is $\alpha_1$ which corresponds to the class size 12-17 and the reference level for $\beta_i$ is $\beta_1$ which corresponds to the school 112038.
- The intercept of 505.8 represents the expected value of the response variable when both i and j are at their reference levels. Specifically, this means the mean of the scaled math score for grade 1 in class size of 12-17 in school 112038 is 505.8
- Notice that the other coefficients for $\alpha_i$'s are negative numbers that are close to each other in magnitude. This finding is relevant to the pattern observed in the noncompliance section, where the analysis indicates a decline in math scores varies based on interval and became stagnant within a certain range. Specifically, from the range "12-17" to "18-21" we see 11 points drop in math score. But the coefficient for the range "22-25" is -12, indicating only 1 point drop in math score as the class size increase from "18-21" to "22-25". This matching pattern continues as we see -17 as the coefficient of the class size >25, indicating 5 points (larger) drop as the class size pass the number 25.
- Similarly, most coefficients for $\beta_j$ are positive, implying higher scores for students attending different schools, with exceptions for 3 school IDs showing negative values, suggesting lower scores.

Conclusion: From the $\alpha_i$ coefficients in the model we can see that past the class size 17, the average math scores show a drastic drop, then showed less drastic change as the class size increases to 25. This observation is consistent with the graph in 6.2.2, and this implies that class sizing has an impact in grade 1 math performance, which is perisitent with our finding, and from this model, 17 seems to be the maximum class size before the students' performance started showing drastic change.

Notably, throughout the analysis, we consistently observed negative coefficients for other $\alpha_i$'s when small class size, and the range "12-17" was used as a reference level. This indicates that small class is the class type that yields the highest math score, which provided an insight to our second question of interest, that is, finding the class type associated with the highest math scaled scores in 1st grade. For more robust investigation, we will compare each group using Tukey HSD test in section 10.5.

#### 10.4 Model Comparison

*Partial Residual Plot*

```{r, echo = FALSE}
par(mfrow = c(2,2))
crPlots(n_model2, terms = "as.factor(g1classtype)", ask = FALSE, xlab = "Model 1", ylab = "Component+Residuals")
crPlots(cc_model, terms = "as.factor(g1classtype)", ask = FALSE, xlab = "Complete Case Model", 
        ylab = "Component+Residuals")
crPlots(model3, terms = "as.factor(g1classtype)", ask = FALSE, xlab = "Model 3", ylab = "Component+Residuals")
crPlots(model4, terms = "as.factor(agg_data$classification2)",ask = FALSE, xlab = "Model 4", ylab = "Component+Residuals")
```

To compare the models we have observed, let's look at their partial residual plot. This plot illustrates the relationship between the response variable and a single predictor variable while adjusting for the effects of other predictors in the model. In this case we want to look at the relationship between average math score and the predictor class type for model 1, complete case model, and model 3 (note that 1= Small, 2 = Regular, 3 = Regular + aide). Whereas we will look at class size as predictor in model 4. 

Notice that across all models, the "small" and "12-17" category consistently exhibits the highest residual median among the different class types, which suggests the existence of systematic difference in the average math scores between students in smaller classes compared to those in other classes, even after accounting for other variables in the model. The higher residual median for the smaller class type indicates that, on average, students in smaller classes tend to have higher math scores compared to what would be predicted based on the model. This finding implies that class size could be a significant predictor of students' math performance, with smaller classes potentially providing better learning environments conducive to higher achievement in mathematics. Hence, answered our first question of interest.

#### 10.5 Secondary Question

A secondary question we aim to answer in this analysis is which class type is associated with the highest math scaled scores in 1st grade. As observed in the analysis above, small classes and the range "12-17" yield a higher average scaled math scores compared to the other categories. However, to be sure of this, we will perform a Tukey HSD test, where we will compare categories (small, regular, regular + aide) to one another.

```{r, echo = FALSE, fig.height = 3, fig,width = 4, fig.align = 'center'}
# Perform Tukey's HSD test
agg_data$g1classtype <- as.factor(agg_data$g1classtype)
t_model <- lm(avg_scores ~ g1classtype, data = agg_data)
anov_t <- aov(t_model)
tukey_result <- TukeyHSD(anov_t)

# Plot the Tukey HSD results
plot(tukey_result, las = 1, col = "brown", cex.axis = 0.7, cex.lab = 1.2, cex.main = 1.4)
```

Using a similar method as mean comparison, we perform another Tukey test that allows us to look at all possible pairwise comparisons, where $H_0 : \mu_i = \mu_j, i \neq j$ and $H_a : \mu_i \neq \mu_j, i \neq j$. Note that 1 = small class, 2 = regular class, 3 = regular + aide

We can observe from the plot that the comparison between the average score of regular and regular + aide classes still crosses the zero line, and it appears that there is a significant difference between small-regular and small-regular + aide. 

```{r, echo = FALSE}
# Display tukey output in a table
options(knitr.kable.NA = '')
tukey_data <- data.frame(
  Groups = c("Regular-Small", "Regular+Aide-Small", "Regular+Aide-Regular"),
  diff = c(-13.541024, -9.455806, 4.085217),
  lwr = c(-21.14885, -17.35412, -3.94989),
  upr = c(-5.933198, -1.557490, 12.120325),
  p_adj = c(0.0001055, 0.0141220, 0.4558354)
)
kable(tukey_data, format = "pandoc", digits = 4, caption = "Tukey HSD table", align = 'c') 
```

We can observe the differences from the Tukey table as well (setting $\alpha = 0.05$):

- The mean math score in regular class is significantly lower (by approximately 13.54 points) than in small class, with a p-value of 0.0001. This suggests that small class is associated with a higher mean math score compared to class 2.
- The mean math score in regular + aide class is significantly lower (by approximately 9.46 points) than in small class, with a p-value of 0.0141. This suggests that small class is associated with a higher mean math score compared to class 3.
- The mean math score in regular + aide class is not significantly different from regular class, as the p-value (0.4558) is greater than the typical significance level of 0.05. This suggests that there is no significant difference in mean math scores between class regular + aide and regular class.

Based on these results, we can conclude that there exist math scores differences between different class types, and small class is associated with the highest mean math score among the three classes. This finding is consistent with what is reflected in the models coefficient above.

### 11. Discussion

#### Findings

Our study rigorously investigated the relationship between class type and math scores, focusing on scaled math score, class type, and school ID as primary variables. Initial variable exploration revealed missing data, issues with heterogeneity, and noncompliance, which were addressed through data imputation and sensitivity analysis. Utilizing a multiple linear regression model,  we observed statistically significant differences in math scores across different class types, with small classes displaying the highest scores. Notably, while small class sizes were advantageous across all school types, inner-city students generally exhibited lower math performance compared to their counterparts. Specifically, their average math score for small classes fell below the average of regular classes in other school locations. Furthermore, our analysis indicated that a class size of 17 appeared to be the optimal threshold before witnessing a notable shift in math score performance. 

The finding that smaller class sizes are associated with higher scores likely reflects the increased individualized attention and support that students receive in such settings, allowing for more personalized instruction and addressing of individual learning needs. However, despite the benefits of smaller classes, inner-city students tend to perform lower than their peers from other areas, possibly due to socio-economic factors and resource disparities that disproportionately affect these schools. This suggests that while class size plays a role in academic outcomes, underlying socio-economic challenges may still influence student performance. Moreover, identifying 17 as the optimal class size before a decline in math score performance suggests a threshold effect, emphasizing the importance of maintaining manageable class sizes to optimize learning conditions and mitigate the impact of socio-economic disparities on student achievement.

#### Caveats in Analysis

1). Replacing free-lunch with the mode of free lunch status per school instead of per class may oversimplify the analysis and obscure individual-level socioeconomic disparities within each class. This approach may not accurately capture the socioeconomic diversity present within each class and could bias the results. Additionally, aggregating free lunch status at the school level may introduce ecological fallacy and lead to misleading interpretations. Therefore, while this approach may offer simplicity, it may not accurately capture the complexities of socioeconomic diversity within each class.

2). Prioritizing interpretability of the untransformed model over minor deviations from normality may appear justified, but it poses risks to the analysis's validity. Relying solely on the Central Limit Theorem (CLT) to assume normality in residuals may overlook deviations from normality, particularly with smaller sample sizes. While linear regression estimates can remain unbiased with minor deviations from normality, extreme departures could lead to biased parameter estimates and inaccurate confidence intervals. Despite its sensitivity to sample size, the Shapiro-Wilk test is valuable for detecting violations of normality assumptions, necessitating further investigation or model adjustments. Thus, while interpretability is crucial, it's vital to balance it with adhering rigorously to statistical assumptions and ensuring the robustness of inferential procedures, particularly where model misspecification could have significant consequences.

3). Additionally, our analysis also opted for a model that emphasized interpretability by including only significant predictors, simplifying the model while potentially overlooking subtle yet meaningful relationships between variables. The simplified model may fail to capture the full complexity of the underlying data, potentially leading to incomplete or biased conclusions. Despite these limitations, this approach facilitates easier interpretation for audience, reduces the risk of overfitting, and enhances the generalizability of the findings. As a future direction, we could explore the inclusion of additional predictors to further understand the factors influencing the response variable, and  achieve a balance between interpretability and model complexity to offer more nuanced insights and advance our understanding of the relationship between math score and class size.

### 12. Conclusion

Our study has provided valuable insights into the relationship between class type and math scores, revealing consistent differences across different class sizes. The findings suggest that smaller class sizes, characterized by more individualized attention and a conducive learning environment, tend to yield higher math scores compared to larger classes. These results underscore the potential benefits of smaller class sizes in enhancing academic achievement and fostering a supportive learning environment. Additionally, our discussion highlighted the importance of careful consideration when handling variables such as free lunch status, normality assumptions, and model selection in statistical analyses. By acknowledging the complexities and potential pitfalls in data analysis, we can ensure that our findings are robust and reliable, contributing to a more comprehensive understanding of the factors influencing educational outcomes.

### Acknowledgement
- Office Hours discussion with TA
- Consultation with Prof. Chen

### References

CES Academy. 4 REASONS WHY SMALL CLASS SIZES LEAD TO BETTER STUDENT PERFORMANCE. https://www.ces-schools.net/4-reasons-why-small-class-sizes-lead-to-better-student-performance/#:~:text=With%20a%20smaller%20class%20size,maneuver%20and%20more%20personal%20space.

Chen, Shinze.Chapter 4 Analysis of Variance. https://nbviewer.org/github/ChenShizhe/StatDataScience/blob/master/Notes/Chapter4ANOVAII.ipynb

C.M. Achilles; Helen Pate Bain; Fred Bellott; Jayne Boyd-Zaharias; Jeremy Finn; John Folger; John Johnston; Elizabeth Word, 2008, “Tennessee’s Student Teacher Achievement Ratio (STAR) project”, https://doi.org/10.7910/DVN/SIWH9F, Harvard Dataverse, V1, UNF:3:Ji2Q+9HCCZAbw3csOdMNdA== [fileUNF]

Dynarski, Susan., & Hyman, Joshua (2013). Experimental Evidence on the Effect of Childhood Investments on Postsecondary Attainment and Degree Completion. Journal of Policy Analysis and Management 32(4). https://www.researchgate.net/publication/228303207_Experimental_Evidence_on_the_Effect_of_Childhood_Investments_on_Postsecondary_Attainment_and_Degree_Completion

Finn, J.D., Gerber, S.B. (2005). Small Classes in the Early Grades, Academic Achievement, and Graduating From High School. Journal of Educational Psychology2005, Vol. 97, No. 2, 214–223.

Gerbing,David. Subset a Data Frame. https://cran.r-project.org/web/packages/lessR/vignettes/Extract.html#:~:text=If%20subsetting%20is%20done%20by,%2C%20d%5B%2Ccols%5D%20.

Hadley,Wickham, et al., A box and whiskers plot (in the style of Tukey). ggplot2. https://ggplot2.tidyverse.org/reference/geom_boxplot.html

Kassambara, Alboukadel. Data Manipulation in R. Data Novia. https://www.datanovia.com/en/lessons/select-data-frame-columns-in-r/

Keyes,David.(2022, March 12). How to Make Beautiful Tables in R. R for the rest of us. https://rfortherestofus.com/2019/11/how-to-make-beautiful-tables-in-r

Krueger, A. B., & Whitmore, D. M. (2001). The Effect of Attending a Small Class in the Early Grades on College-Test Taking and Middle School Test Results: Evidence from Project STAR. The Economic Journal, 111(468), 1–28. http://www.jstor.org/stable/2667840

Lane, D. (2010). Tukey’s honestly significant difference (hsd). In Encyclopedia of Research Design (Vol. 0, pp. 1566-1570). SAGE Publications, Inc., https://doi.org/10.4135/9781412961288

National Center for Education Statistics. Status and Trends in the Education of Racial and Ethnic Minorities. https://nces.ed.gov/pubs2010/2010015/tables/table_1a.asp

Statistical tools for high-throughput data analysis. (n.d.) Normality Test in R. https://owl.purdue.edu/owl/research_and_citation/apa_style/apa_formatting_and_style_guide/reference_list_electronic_sources.html

Toth, Michael. (May 1, 2019). Detailed Guide to the Bar Chart in R with ggplot. R-bloggers. https://www.r-bloggers.com/2019/05/detailed-guide-to-the-bar-chart-in-r-with-ggplot/

Zach. (2020). The Breusch-Pagan Test: Definition & Example. Statology. https://www.statology.org/breusch-pagan-test/#:~:text=The%20test%20uses%20the%20following,not%20distributed%20with%20equal%20variance)


### Code Appendix
```{r getlabels, echo = FALSE}
labs = knitr::all_labels()
labs = labs[!labs %in% c("setup","getlabels","allcode")]
```

```{r all code, ref.label = labs, eval = FALSE}
```